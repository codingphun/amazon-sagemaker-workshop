{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Run a Ground Truth labeling job (time: about 3h)](#Run-a-Ground-Truth-labeling-job)\n",
    "    1. [Prepare the data](#Prepare-the-data)\n",
    "    2. [Specify the categories](#Specify-the-categories)\n",
    "    3. [Create the instruction template](#Create-the-instruction-template)\n",
    "    4. [Create a private team](#Create-a-private-team)\n",
    "    5. [Define pre-built lambda functions for use in the labeling job](#Define-pre-built-lambda-functions-for-use-in-the-labeling-job)\n",
    "    6. [Submit the Ground Truth job request](#Submit-the-Ground-Truth-job-request)\n",
    "    7. [Monitor job progress](#Monitor-job-progress)\n",
    "3. [Analyze Ground Truth labeling job results](#Analyze-Ground-Truth-labeling-job-results)\n",
    "    1. [Postprocess the output manifest](#Postprocess-the-output-manifest)\n",
    "    2. [Plot class histograms](#Plot-class-histograms)\n",
    "    3. [Plot annotated images](#Plot-annotated-images)\n",
    "        1. [Plot a small output sample](#Plot-a-small-output-sample)\n",
    "4. [Compare Ground Truth results to to know, pre-labeled data](#Compare-Ground-Truth-results-to-known,-pre-labeled-data)\n",
    "    1. [Compute accuracy](#Compute-accuracy)\n",
    "    2. [Plot correct and incorrect annotations](#Plot-correct-and-incorrect-annotations)\n",
    "5. [Train an image classifier using Ground Truth labels](#Train-an-image-classifier-using-Ground-Truth-labels)\n",
    "6. [Deploy the Model](#Deploy-the-Model)\n",
    "    1. [Create Model](#Create-Model)\n",
    "    2. [Batch Transform](#Batch-Transform)\n",
    "    3. [Realtime Inference](#Realtime-Inference)\n",
    "        1. [Create Endpoint Configuration](#Create-Endpoint-Configuration)\n",
    "        2. [Create Endpoint](#Create-Endpoint)\n",
    "        3. [Perform Inference](#Perform-Inference)\n",
    "7. [Review](#Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This sample notebook takes you through an end-to-end workflow to demonstrate the functionality of SageMaker Ground Truth. We'll start with an unlabeled image data set, acquire labels for all the images using SageMaker Ground Truth, analyze the results of the labeling job, train an image classifier, host the resulting model, and, finally, use it to make predictions. Before you begin, we highly recommend you start a Ground Truth labeling job through the AWS Console first to familiarize yourself with the workflow. The AWS Console offers less flexibility than the API, but is simple to use.\n",
    "\n",
    "In the interest of time and cost, you will create a private workforce, add yourself to the workforce and then label 100 images yourself. The trained model will not production ready on this very limited dataset but you should still be able to see good results. In the real world, you should have a much larger labeled datasets and you invite more workers to your private workforce to help you. You can also use Ground Truth public workforce with Amazon Mechanical Turk. The public workforce is a world-wide resource. Workers are available 24 hours a day, 7 days a week. You typically get the fastest turn-around for your labeling jobs when you use the public workforce. Note that the public workforce is a world-wide resource. Workers are available 24 hours a day, 7 days a week. You typically get the fastest turn-around for your labeling jobs when you use the public workforce. \n",
    "\n",
    "You can also use Ground Truthâ€™s auto-labeling feature which learn from human responses and automatically create labels for the easiest images at a lower cost. However, keep in mind that auto-labeling feature only turns on for a larger dataset of more than 1000 images. \n",
    "\n",
    "\n",
    "#### Prerequisites\n",
    "To run this notebook, you can simply execute each cell one-by-one. To understand what's happening, you'll need:\n",
    "* An S3 bucket you can write to -- please provide its name in the following cell. The bucket must be in the same region as this SageMaker Notebook instance. You can also change the `EXP_NAME` to any valid S3 prefix. All the files related to this experiment will be stored in that prefix of your bucket.\n",
    "* Familiarity with Python and [numpy](http://www.numpy.org/).\n",
    "* Basic familiarity with [AWS S3](https://docs.aws.amazon.com/s3/index.html),\n",
    "* Basic understanding of [AWS Sagemaker](https://aws.amazon.com/sagemaker/),\n",
    "* Basic familiarity with [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/) -- set it up with credentials to access the AWS account you're running this notebook from. This should work out-of-the-box on SageMaker Jupyter Notebook instances.\n",
    "\n",
    "This notebook is only tested on a SageMaker notebook instance. The runtimes given are approximate, we used an `ml.m4.xlarge` instance in our tests. \n",
    "\n",
    "NOTE: This notebook will create/remove subdirectories in its working directory. We recommend to place this notebook in its own directory before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import boto3\n",
    "import sagemaker\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "BUCKET = '<< YOUR S3 BUCKET NAME >>'\n",
    "assert BUCKET != '<< YOUR S3 BUCKET NAME >>', 'Please provide a custom S3 bucket name.'\n",
    "EXP_NAME = 'ground-truth-workshop' # Any valid S3 prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the bucket is in the same region as this notebook.\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client('s3')\n",
    "bucket_region = s3.head_bucket(Bucket=BUCKET)['ResponseMetadata']['HTTPHeaders']['x-amz-bucket-region']\n",
    "assert bucket_region == region, \"You S3 bucket {} and this notebook need to be in the same region.\".format(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Ground Truth labeling job\n",
    "\n",
    "\n",
    "We will first run a labeling job. This involves several steps: collecting the images we want labeled, specifying the possible label categories, creating instructions, and writing a labeling job specification. \n",
    "\n",
    "## Prepare the data\n",
    "We will first download images and labels of a subset of the [Google Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html). These labels were [carefully verified](https://storage.googleapis.com/openimages/web/factsfigures.html). Later, will compare Ground Truth annotations to these labels. Our dataset will include images in the following categories:\n",
    "\n",
    "* Musical Instrument (500 images)\n",
    "* Fruit (370 images)\n",
    "* Cheetah (50 images)\n",
    "* Tiger (40 images)\n",
    "* Snowman (40 images)\n",
    "\n",
    "If you chose `RUN_FULL_AL_DEMO = False`, then we will choose a subset of 100 images in this dataset. This is a diverse dataset of interesting images, and should be fun for the human annotators to work with. We will copy these images to our local `BUCKET`, and will create the corresponding *input manifest*. The input manifest is a formatted list of the S3 locations of the images we want Ground Truth to annotate. We will upload this manifest to our S3 `BUCKET`.\n",
    "\n",
    "#### Disclosure regarding the Open Images Dataset V4:\n",
    "Open Images Dataset V4 is created by Google Inc. We have not modified the images or the accompanying annotations. You can obtain the images and the annotations [here](https://storage.googleapis.com/openimages/web/download.html). The annotations are licensed by Google Inc. under [CC BY 4.0](https://creativecommons.org/licenses/by/2.0/) license. The images are listed as having a [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/) license. The following paper describes Open Images V4 in depth: from the data collection and annotation to detailed statistics about the data and evaluation of models trained on it.\n",
    "\n",
    "A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig, and V. Ferrari.\n",
    "*The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale.* arXiv:1811.00982, 2018. ([link to PDF](https://arxiv.org/abs/1811.00982))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Download and process the Open Images annotations.\n",
    "!wget https://storage.googleapis.com/openimages/2018_04/test/test-annotations-human-imagelabels-boxable.csv -O openimgs-annotations.csv\n",
    "with open('openimgs-annotations.csv', 'r') as f:\n",
    "    all_labels = [line.strip().split(',') for line in f.readlines()]\n",
    "\n",
    "# Extract image ids in each of our desired classes.\n",
    "ims = {}\n",
    "ims['Musical Instrument'] = [label[0] for label in all_labels if (label[2] == '/m/04szw' and label[3] == '1')][:500]\n",
    "ims['Fruit'] = [label[0] for label in all_labels if (label[2] == '/m/02xwb' and label[3] == '1')][:371]\n",
    "ims['Fruit'].remove('02a54f6864478101') # This image contains personal information, let's remove it from our dataset.\n",
    "ims['Cheetah'] = [label[0] for label in all_labels if (label[2] == '/m/0cd4d' and label[3] == '1')][:50]\n",
    "ims['Tiger'] = [label[0] for label in all_labels if (label[2] == '/m/07dm6' and label[3] == '1')][:40]\n",
    "ims['Snowman'] = [label[0] for label in all_labels if (label[2] == '/m/0152hh' and label[3] == '1')][:40]\n",
    "num_classes = len(ims)\n",
    "\n",
    "# reduce each class count 10 times.\n",
    "for key in ims.keys():\n",
    "    ims[key] = set(ims[key][:int(len(ims[key]) / 10)])\n",
    "\n",
    "# Copy the images to our local bucket.\n",
    "s3 = boto3.client('s3')\n",
    "for img_id, img in enumerate(itertools.chain.from_iterable(ims.values())):\n",
    "    if (img_id + 1) % 10 == 0:\n",
    "        print('Copying image {} / {}'.format((img_id+1), 100))\n",
    "    copy_source = {\n",
    "        'Bucket': 'open-images-dataset',\n",
    "        'Key': 'test/{}.jpg'.format(img)\n",
    "    }\n",
    "    s3.copy(copy_source, BUCKET, '{}/images/{}.jpg'.format(EXP_NAME, img))\n",
    "\n",
    "# Create and upload the input manifest.\n",
    "manifest_name = 'input.manifest'\n",
    "with open(manifest_name, 'w') as f:\n",
    "    for img in itertools.chain.from_iterable(ims.values()):\n",
    "        img_path = 's3://{}/{}/images/{}.jpg'.format(BUCKET, EXP_NAME, img)\n",
    "        f.write('{\"source-ref\": \"' + img_path +'\"}\\n')\n",
    "s3.upload_file(manifest_name, BUCKET, EXP_NAME + '/' + manifest_name)\n",
    "print ('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, you should be able to go to `s3://BUCKET/EXP_NAME/images` in [S3 console](https://console.aws.amazon.com/s3/) and see a thousand images. We recommend you inspect the contents of these images! You can also download them all to a local machine using the AWS CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the categories\n",
    "To run an image classification labeling job, you need to decide on a set of classes the annotators can choose from. \n",
    "In our case, this list is `[\"Musical Instrument\", \"Fruit\", \"Cheetah\", \"Tiger\", \"Snowman\"]`. In your own job you can choose any list of up to 10 classes. We recommend the classes to be as unambiguous and concrete as possible. The categories should be mutually exclusive, with only one correct label per image. In addition, be careful to make the task as *objective* as possible, unless of course your intention is to obtain subjective labels.\n",
    "* Example good category lists: `[\"Human\", \"No Human\"]`, `[\"Golden Retriever\", \"Labrador\", \"English Bulldog\", \"German Shepherd\"]`, `[\"Car\", \"Train\", \"Ship\", \"Pedestrian\"]`.\n",
    "* Example bad category lists: `[\"Prominent object\", \"Not prominent\"]` (meaning unclear), `[\"Beautiful\", \"Ugly\"]` (subjective), `[\"Dog\", \"Animal\", \"Car\"]` (not mutually exclusive). \n",
    "\n",
    "To work with Ground Truth, this list needs to be converted to a .json file and uploaded to the S3 `BUCKET`.\n",
    "\n",
    "*Note: The ordering of the labels or classes in the template governs the class indices that you will see downstream in the output manifest (this numbering is zero-indexed). In other words, the class that appears second in the template will correspond to class \"1\" in the output. At the end of this demonstration, we will train a model and make predictions, and this class ordering is instrumental to interpreting the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LIST = list(ims.keys())\n",
    "print(\"Label space is {}\".format(CLASS_LIST))\n",
    "\n",
    "json_body = {\n",
    "    'labels': [{'label': label} for label in CLASS_LIST]\n",
    "}\n",
    "with open('class_labels.json', 'w') as f:\n",
    "    json.dump(json_body, f)\n",
    "    \n",
    "s3.upload_file('class_labels.json', BUCKET, EXP_NAME + '/class_labels.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see `class_labels.json` in `s3://BUCKET/EXP_NAME/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the instruction template\n",
    "Part or all of your images will be annotated by human annotators. It is **essential** to provide good instructions that help the annotators give you the annotations you want. Good instructions are:\n",
    "1. Concise. We recommend limiting verbal/textual instruction to two sentences, and focusing on clear visuals.\n",
    "2. Visual. In the case of image classification, we recommend providing one labeled image in each of the classes as part of the instruction.\n",
    "\n",
    "When used through the AWS Console, Ground Truth helps you create the instructions using a visual wizard. When using the API, you need to create an HTML template for your instructions. Below, we prepare a very simple but effective template and upload it to your S3 bucket.\n",
    "\n",
    "**NOTE**: If you use any images in your template (as we do), they need to be publicly accessible. You can enable public access to files in your S3 bucket through the S3 Console, as described in [S3 Documentation](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-object-permissions.html). \n",
    "\n",
    "#### Testing your instructions\n",
    "It is very easy to create broken instructions. This might cause your labeling job to fail. However, it might also cause your job to complete with meaningless results (when the annotators have no idea what to do, or the instructions are plain wrong). We *highly recommend* that you verify that your task is correct in two ways:\n",
    "1. The following cell creates and uploads a file called `instructions.template` to S3. It also creates `instructions.html` that you can open in a local browser window. Please do so and inspect the resulting web page; it should correspond to what you want your annotators to see (except the actual image to annotate will not be visible).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_examples = ['https://s3.amazonaws.com/open-images-dataset/test/{}'.format(img_id)\n",
    "                for img_id in ['0634825fc1dcc96b.jpg', '0415b6a36f3381ed.jpg', '8582cc08068e2d0f.jpg', '8728e9fa662a8921.jpg', '926d31e8cde9055e.jpg']]\n",
    "\n",
    "def make_template(test_template=False, save_fname='instructions.template'):\n",
    "    template = r\"\"\"<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "    <crowd-form>\n",
    "      <crowd-image-classifier\n",
    "        name=\"crowd-image-classifier\"\n",
    "        src=\"{{{{ task.input.taskObject | grant_read_access }}}}\"\n",
    "        header=\"Dear Annotator, please tell me what you can see in the image. Thank you!\"\n",
    "        categories=\"{categories_str}\"\n",
    "      >\n",
    "        <full-instructions header=\"Image classification instructions\">\n",
    "        </full-instructions>\n",
    "\n",
    "        <short-instructions>\n",
    "          <p>Dear Annotator, please tell me whether what you can see in the image. Thank you!</p>\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Musical Instrument\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Fruit\".</p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Cheetah\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Tiger\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Snowman\". </p>\n",
    "\n",
    "        </short-instructions>\n",
    "\n",
    "      </crowd-image-classifier>\n",
    "    </crowd-form>\"\"\".format(*img_examples,\n",
    "                           categories_str=str(CLASS_LIST) if test_template else '{{ task.input.labels | to_json | escape }}')\n",
    "\n",
    "    with open(save_fname, 'w') as f:\n",
    "        f.write(template)\n",
    "    if test_template is False:\n",
    "        print(template)\n",
    "        \n",
    "make_template(test_template=True, save_fname='instructions.html')\n",
    "make_template(test_template=False, save_fname='instructions.template')\n",
    "s3.upload_file('instructions.template', BUCKET, EXP_NAME + '/instructions.template')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to find your template in `s3://BUCKET/EXP_NAME/instructions.template`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a private team\n",
    "\n",
    "This step requires you to use the AWS Console. \n",
    "\n",
    "We will create a `private team` and add only one user (you) to it. Then, we will modify the Ground Truth API job request to send the task to that workforce. \n",
    "\n",
    "To create a private team:\n",
    "1. Go to `AWS Console > Amazon SageMaker > Labeling workforces`\n",
    "2. Click \"Private\" and then \"Create private team\". \n",
    "3. Enter the desired name for your private workteam.\n",
    "4. Enter your own email address in the \"Email addresses\" section. \n",
    "5. Enter the name of your organization and a contact email to administrate the private workteam.\n",
    "6. Click \"Create Private Team\".\n",
    "7. The AWS Console should now return to `AWS Console > Amazon SageMaker > Labeling workforces`. Your newly created team should be visible under \"Private teams\". Next to it you will see an `ARN` which is a long string that looks like `arn:aws:sagemaker:region-name-123456:workteam/private-crowd/team-name`. Copy this ARN in the cell below.\n",
    "8. You should get an email from `no-reply@verificationemail.com` that contains your workforce username and password. \n",
    "9. In `AWS Console > Amazon SageMaker > Labeling workforces`, click on the URL in `Labeling portal sign-in URL`. Use the email/password combination from Step 8 to log in (you will be asked to create a new, non-default password).\n",
    "\n",
    "That's it! This is your private worker's interface. When you submit Groud Truth job request, your task should appear in this window. You can invite your colleagues to participate in the labeling job by clicking the \"Invite new workers\" button.\n",
    "\n",
    "The [SageMaker Ground Truth documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management-private.html) has more details on the management of private workteams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_workteam_arn = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pre-built lambda functions for use in the labeling job \n",
    "\n",
    "Before we submit the request, we need to define the ARNs for four key components of the labeling job: 1) the workteam, 2) the annotation consolidation Lambda function, 3) the pre-labeling task Lambda function, and 4) the machine learning algorithm to perform auto-annotation. These functions are defined by strings with region names and AWS service account numbers, so we will define a mapping below that will enable you to run this notebook in any of our supported regions. \n",
    "\n",
    "See the official documentation for the available ARNs:\n",
    "* [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management-public.html) for a discussion of the workteam ARN definition. There is only one valid selection if you choose to use the public workfofce; if you elect to use a private workteam, you should check the corresponding ARN for the workteam.\n",
    "* [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/API_HumanTaskConfig.html#SageMaker-Type-HumanTaskConfig-PreHumanTaskLambdaArn) for available pre-human ARNs for other workflows.\n",
    "* [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/API_AnnotationConsolidationConfig.html#SageMaker-Type-AnnotationConsolidationConfig-AnnotationConsolidationLambdaArn) for available annotation consolidation ANRs for other workflows.\n",
    "* [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/API_LabelingJobAlgorithmsConfig.html#SageMaker-Type-LabelingJobAlgorithmsConfig-LabelingJobAlgorithmSpecificationArn) for available auto-labeling ARNs for other workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify ARNs for resources needed to run an image classification job.\n",
    "ac_arn_map = {'us-west-2': '081040173940',\n",
    "              'us-east-1': '432418664414',\n",
    "              'us-east-2': '266458841044',\n",
    "              'eu-west-1': '568282634449',\n",
    "              'ap-northeast-1': '477331159723'}\n",
    "\n",
    "prehuman_arn = 'arn:aws:lambda:{}:{}:function:PRE-ImageMultiClass'.format(region, ac_arn_map[region])\n",
    "acs_arn = 'arn:aws:lambda:{}:{}:function:ACS-ImageMultiClass'.format(region, ac_arn_map[region]) \n",
    "labeling_algorithm_specification_arn = 'arn:aws:sagemaker:{}:027400017018:labeling-job-algorithm-specification/image-classification'.format(region)\n",
    "workteam_arn = 'arn:aws:sagemaker:{}:394669845002:workteam/public-crowd/default'.format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the Ground Truth job request\n",
    "The API starts a Ground Truth job by submitting a request. The request contains the \n",
    "full configuration of the annotation task, and allows you to modify the fine details of\n",
    "the job that are fixed to default values when you use the AWS Console. The parameters that make up the request are described in more detail in the [SageMaker Ground Truth documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateLabelingJob.html).\n",
    "\n",
    "After you submit the request, you should be able to see the job in your AWS Console, at `Amazon SageMaker > Labeling Jobs`.\n",
    "You can track the progress of the job there. \n",
    "\n",
    "In a nutshell, the job will proceed in multiple iterations.\n",
    "\n",
    "* Iteration 1: Ground Truth will send out 10 images as 'probes' for human annotation. If these are succesfully annotated, proceed to Iteration 2.\n",
    "* Iteration 2: Send out a batch of `MaxConcurrentTaskCount - 10` (in our case, 90) images for human annotation to obtain an active learning training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = 'What do you see: a {}?'.format(' a '.join(CLASS_LIST))\n",
    "task_keywords = ['image', 'classification', 'humans']\n",
    "task_title = task_description\n",
    "job_name = 'ground-truth-workshop-' + str(int(time.time()))\n",
    "\n",
    "human_task_config = {\n",
    "      \"AnnotationConsolidationConfig\": {\n",
    "        \"AnnotationConsolidationLambdaArn\": acs_arn,\n",
    "      },\n",
    "      \"PreHumanTaskLambdaArn\": prehuman_arn,\n",
    "      \"MaxConcurrentTaskCount\": 100, # 100 images will be sent at a time to the workteam.\n",
    "      \"NumberOfHumanWorkersPerDataObject\": 1, # 3 separate workers will be required to label each image.\n",
    "      \"TaskAvailabilityLifetimeInSeconds\": 21600, # Your worteam has 6 hours to complete all pending tasks.\n",
    "      \"TaskDescription\": task_description,\n",
    "      \"TaskKeywords\": task_keywords,\n",
    "      \"TaskTimeLimitInSeconds\": 300, # Each image must be labeled within 5 minutes.\n",
    "      \"TaskTitle\": task_title,\n",
    "      \"UiConfig\": {\n",
    "        \"UiTemplateS3Uri\": 's3://{}/{}/instructions.template'.format(BUCKET, EXP_NAME),\n",
    "      }\n",
    "    }\n",
    "human_task_config[\"WorkteamArn\"] = private_workteam_arn\n",
    "\n",
    "ground_truth_request = {\n",
    "        \"InputConfig\" : {\n",
    "          \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "              \"ManifestS3Uri\": 's3://{}/{}/{}'.format(BUCKET, EXP_NAME, manifest_name),\n",
    "            }\n",
    "          },\n",
    "          \"DataAttributes\": {\n",
    "            \"ContentClassifiers\": [\n",
    "              \"FreeOfPersonallyIdentifiableInformation\",\n",
    "              \"FreeOfAdultContent\"\n",
    "            ]\n",
    "          },  \n",
    "        },\n",
    "        \"OutputConfig\" : {\n",
    "          \"S3OutputPath\": 's3://{}/{}/output/'.format(BUCKET, EXP_NAME),\n",
    "        },\n",
    "        \"HumanTaskConfig\" : human_task_config,\n",
    "        \"LabelingJobName\": job_name,\n",
    "        \"RoleArn\": role, \n",
    "        \"LabelAttributeName\": \"category\",\n",
    "        \"LabelCategoryConfigS3Uri\": 's3://{}/{}/class_labels.json'.format(BUCKET, EXP_NAME),\n",
    "    }\n",
    "    \n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_client.create_labeling_job(**ground_truth_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Label!\n",
    "\n",
    "Go to Ground Truth Private workforce page and click on **Labeling portal sign-in URL** to sign in to labeling portal. You should see a task waiting for you to label, if not, wait for a few minutes and refresh the page. \n",
    "\n",
    "This step should take about 20 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor job progress\n",
    "You can monitor the job's progress through AWS Console but in this notebook, we will use Ground Truth output files and Cloud Watch logs in order to monitor the progress. \n",
    "\n",
    "You can re-evaluate the next cell repeatedly. It sends a `describe_labelging_job` request which should tell you whether the job is completed or not. If it is, then 'LabelingJobStatus' will be 'Completed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.describe_labeling_job(LabelingJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Ground Truth labeling job results\n",
    "\n",
    "After the job finishes running (**make sure `sagemaker_client.describe_labeling_job` shows the job is complete!**), it is time to analyze the results. \n",
    "\n",
    "In this section, we will gain additional insights into the results, all contained in the `output manifest`. You can find the location of the output manifest under `AWS Console > SageMaker > Labeling Jobs > [name of your job]`. We will obtain it programmatically in the cell below.\n",
    "\n",
    "## Postprocess the output manifest\n",
    "Now that the job is complete, we will download the output manifest manfiest and postprocess it to form four arrays:\n",
    "* `img_uris` contains the S3 URIs of all the images that Ground Truth annotated.\n",
    "* `labels` contains Ground Truth's labels for each image in `img_uris`.\n",
    "* `confidences` contains the confidence of each label in `labels`.\n",
    "* `human` is a flag array that contains 1 at indices corresponding to images annotated by human annotators, and 0 at indices corresponding to images annotated by Ground Truth's automated data labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the output manifest's annotations.\n",
    "OUTPUT_MANIFEST = 's3://{}/{}/output/{}/manifests/output/output.manifest'.format(BUCKET, EXP_NAME, job_name)\n",
    "\n",
    "!aws s3 cp {OUTPUT_MANIFEST} 'output.manifest'\n",
    "\n",
    "with open('output.manifest', 'r') as f:\n",
    "    output = [json.loads(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# Create data arrays.\n",
    "img_uris = [None] * len(output)\n",
    "confidences = np.zeros(len(output))\n",
    "groundtruth_labels = [None] * len(output)\n",
    "human = np.zeros(len(output))\n",
    "\n",
    "# Find the job name the manifest corresponds to.\n",
    "keys = list(output[0].keys())\n",
    "metakey = keys[np.where([('-metadata' in k) for k in keys])[0][0]]\n",
    "jobname = metakey[:-9]\n",
    "\n",
    "# Extract the data.\n",
    "for datum_id, datum in enumerate(output):\n",
    "    img_uris[datum_id] = datum['source-ref']\n",
    "    groundtruth_labels[datum_id] = str(datum[metakey]['class-name'])\n",
    "    confidences[datum_id] = datum[metakey]['confidence']\n",
    "    human[datum_id] = int(datum[metakey]['human-annotated'] == 'yes')\n",
    "groundtruth_labels = np.array(groundtruth_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot class histograms\n",
    "Now, let's plot the class histograms. The next cell should produce three subplots:\n",
    "* The Left subplot shows the number of images annotated as belonging to each visual category. The categories will be sorted from the most to the least numerous. Each bar is divided into a 'human' and 'machine' part which shows how many images were annotated as given category by human annotators and by the automated data labeling mechanism.\n",
    "\n",
    "* The Middle subplot is the same as Left, except y-axis is in log-scale. This helps visualize unbalanced datasets where some categories contain orders of magnitude more images than other.\n",
    "\n",
    "* The Right subplot shows the average confidence of images in each category, separately for human and auto-annotated images.\n",
    "\n",
    "Note that because you labeled only 100 images by hand, Ground Truth did not auto label any of the images so you will not see any chart for machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of annotations in each class.\n",
    "n_classes = len(set(groundtruth_labels))\n",
    "sorted_clnames, class_sizes = zip(*Counter(groundtruth_labels).most_common(n_classes))\n",
    "\n",
    "# Find ids of human-annotated images.\n",
    "human_sizes = [human[groundtruth_labels == clname].sum() for clname in sorted_clnames]\n",
    "class_sizes = np.array(class_sizes)\n",
    "human_sizes = np.array(human_sizes)\n",
    "\n",
    "# Compute the average annotation confidence per class.\n",
    "human_confidences = np.array([confidences[np.logical_and(groundtruth_labels == clname, human)]\n",
    "                              for clname in sorted_clnames])\n",
    "machine_confidences = [confidences[np.logical_and(groundtruth_labels == clname, 1-human)]\n",
    "                       for clname in sorted_clnames]\n",
    "\n",
    "# If there is no images annotated as a specific class, set the average class confidence to 0.\n",
    "for class_id in range(n_classes):\n",
    "    if human_confidences[class_id].size == 0:\n",
    "        human_confidences[class_id] = np.array([0])\n",
    "    if machine_confidences[class_id].size == 0:\n",
    "        machine_confidences[class_id] = np.array([0])\n",
    "\n",
    "plt.figure(figsize=(9, 3), facecolor='white', dpi=100)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Annotation histogram')\n",
    "plt.bar(range(n_classes), human_sizes, color='gray', hatch='/', edgecolor='k', label='human')\n",
    "plt.bar(range(n_classes), class_sizes - human_sizes, bottom=human_sizes, color='gray', edgecolor='k', label='machine')\n",
    "plt.xticks(range(n_classes), sorted_clnames, rotation=90)\n",
    "plt.ylabel('Annotation Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Annotation histogram (logscale)')\n",
    "plt.bar(range(n_classes), human_sizes, color='gray', hatch='/', edgecolor='k', label='human')\n",
    "plt.bar(range(n_classes), class_sizes - human_sizes, bottom=human_sizes, color='gray', edgecolor='k', label='machine')\n",
    "plt.xticks(range(n_classes), sorted_clnames, rotation=90)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Mean confidences')\n",
    "plt.bar(np.arange(n_classes), [conf.mean() for conf in human_confidences],\n",
    "        color='gray', hatch='/', edgecolor='k', width=.4)\n",
    "plt.bar(np.arange(n_classes) + .4, [conf.mean() for conf in machine_confidences],\n",
    "        color='gray', edgecolor='k', width=.4)\n",
    "plt.xticks(range(n_classes), sorted_clnames, rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot annotated images\n",
    "In any data science task, it is crucial to plot and inspect the results to check they make sense. In order to do this, we will \n",
    "1. Download the input images that Ground Truth annotated.\n",
    "2. Split them by annotated category and whether the annotation was done by human or the auto-labeling mechanism.\n",
    "3. Plot images in each category and human/auto-annoated class.\n",
    "\n",
    "We will download the input images to `LOCAL_IMAGE_DIR` you can choose in the next cell. Note that if this directory already contains images with the same filenames as your Ground Truth input images, we will not re-download the images.\n",
    "\n",
    "If your dataset is large and you do not wish to download and plot **all** the images, simply set `DATASET_SIZE` to a small number. We will pick a random subset of your data for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_IMG_DIR = 'localimgdir' # Replace with the name of a local directory to store images.\n",
    "assert LOCAL_IMG_DIR != '<< choose a local directory name to download the images to >>', 'Please provide a local directory name'\n",
    "DATASET_SIZE = len(img_uris) # Change this to a reasonable number if your dataset much larger than 10K images.\n",
    "\n",
    "subset_ids = np.random.choice(range(len(img_uris)), DATASET_SIZE, replace=False)\n",
    "img_uris = [img_uris[idx] for idx in subset_ids]\n",
    "groundtruth_labels = groundtruth_labels[subset_ids]\n",
    "confidences = confidences[subset_ids]\n",
    "human = human[subset_ids]\n",
    "\n",
    "img_fnames = [None] * len(output)\n",
    "for img_uri_id, img_uri in enumerate(img_uris):\n",
    "    target_fname = os.path.join(\n",
    "        LOCAL_IMG_DIR, img_uri.split('/')[-1])\n",
    "    if not os.path.isfile(target_fname):\n",
    "        !aws s3 cp {img_uri} {target_fname}\n",
    "    img_fnames[img_uri_id] = target_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a small output sample\n",
    "The following cell will create two figures. The first plots `N_SHOW` images in each category, as annotated by humans. The second plots `N_SHOW` images in each category, as annotated by the auto-labeling mechanism. \n",
    "\n",
    "If any category contains less than `N_SHOW` images, that row will not be displayed. By default, `N_SHOW = 10`, but feel free to change this to any other small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_SHOW = 10\n",
    "\n",
    "plt.figure(figsize=(3 * N_SHOW, 2 + 3 * n_classes), facecolor='white', dpi=60)\n",
    "for class_name_id, class_name in enumerate(sorted_clnames):\n",
    "    class_ids = np.where(np.logical_and(np.array(groundtruth_labels) == class_name, human))[0]\n",
    "    try:\n",
    "        show_ids = class_ids[:N_SHOW]\n",
    "    except ValueError:\n",
    "        print('Not enough human annotations to show for class: {}'.format(class_name))\n",
    "        continue\n",
    "    for show_id_id, show_id in enumerate(show_ids):\n",
    "        plt.subplot2grid((n_classes, N_SHOW), (class_name_id, show_id_id))\n",
    "        plt.title('Human Label: ' + class_name)\n",
    "        plt.imshow(imageio.imread(img_fnames[show_id])) #image_fnames\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.figure(figsize=(3 * N_SHOW, 2 + 3 * n_classes), facecolor='white', dpi=100)\n",
    "for class_name_id, class_name in enumerate(sorted_clnames):\n",
    "    class_ids = np.where(np.logical_and(np.array(groundtruth_labels) == class_name, 1-human))[0]\n",
    "    try:\n",
    "        show_ids = np.random.choice(class_ids, N_SHOW, replace=False)\n",
    "    except ValueError:\n",
    "        print('Not enough machine annotations to show for class: {}'.format(class_name))\n",
    "        continue\n",
    "    for show_id_id, show_id in enumerate(show_ids):\n",
    "        plt.subplot2grid((n_classes, N_SHOW), (class_name_id, show_id_id))\n",
    "        plt.title('Auto Label: ' + class_name)\n",
    "        plt.imshow(imageio.imread(img_fnames[show_id]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Ground Truth results to known, pre-labeled data\n",
    "\n",
    "Sometimes (for example, when benchmarking the system) we have an alternative set of data labels available. \n",
    "For example, the Open Images data has already been carefully annotated by a professional annotation workforce.\n",
    "This allows us to perform additional analysis that compares Ground Truth labels to the known, pre-labeled data.\n",
    "When doing so, it is important to bear in mind that any image labels created by humans\n",
    "will most likely not be 100% accurate. For this reason, it is better to think of labeling accuracy as\n",
    "\"adherence to a particular standard / set of labels\" rather than \"how good (in absolute terms) are the Ground Truth labels.\"\n",
    "\n",
    "## Compute accuracy\n",
    "In this cell, we will calculate the accuracy of Ground Truth labels with respect to the standard labels. \n",
    "\n",
    "In [Prepare the data](#Prepare-the-data), we created the `ims` dictionary that specifies which image belongs to each category.\n",
    "We will convert it to an array `standard_labels` such that `standard_labels[i]` contains the label of the `i-th` image, and\n",
    "should ideally correspond to `groundtruth_labels[i]`.\n",
    "\n",
    "This will allow us to plot confusion matrices to assess how well the Ground Truth labels adhere to the standard labels. We plot a confusion matrix for the total dataset, and separate matrices for human annotations and auto-annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', normalize=False, cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j].astype(int), fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Convert the 'ims' dictionary (which maps class names to images) to a list of image classes.\n",
    "standard_labels = []\n",
    "for img_uri in img_uris:\n",
    "    img_uri = img_uri.split('/')[-1].split('.')[0]\n",
    "    standard_label = [cname for cname, imgs_in_cname in ims.items() if img_uri in imgs_in_cname][0]\n",
    "    standard_labels.append(standard_label)\n",
    "standard_labels = np.array(standard_labels)\n",
    "\n",
    "# Plot a confusion matrix for the full dataset.\n",
    "plt.figure(facecolor='white', figsize=(12, 4), dpi=100)\n",
    "plt.subplot(131)\n",
    "mean_err = 100 - np.mean(standard_labels == groundtruth_labels) * 100\n",
    "cnf_matrix = confusion_matrix(standard_labels, groundtruth_labels)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=sorted(ims.keys()),\n",
    "                      title='Full annotation set error {:.2f}%'.format(\n",
    "                          mean_err), normalize=False)\n",
    "\n",
    "# Plot a confusion matrix for human-annotated Ground Truth labels.\n",
    "plt.subplot(132)\n",
    "mean_err = 100 - np.mean(standard_labels[human==1.] == groundtruth_labels[human==1.]) * 100\n",
    "cnf_matrix = confusion_matrix(standard_labels[human==1.], groundtruth_labels[human==1.])\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=sorted(ims.keys()),\n",
    "                      title='Human annotation set (size {}) error {:.2f}%'.format(\n",
    "                          int(sum(human)), mean_err), normalize=False)\n",
    "\n",
    "# Plot a confusion matrix for auto-annotated Ground Truth labels.\n",
    "if sum(human==0.) > 0:\n",
    "    plt.subplot(133)\n",
    "    mean_err = 100 - np.mean(standard_labels[human==0.] == groundtruth_labels[human==0.]) * 100\n",
    "    cnf_matrix = confusion_matrix(standard_labels[human==0.], groundtruth_labels[human==0.])\n",
    "    np.set_printoptions(precision=2)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=sorted(ims.keys()),\n",
    "                          title='Auto-annotation set (size {}) error {:.2f}%'.format(\n",
    "                              int(len(human) - sum(human)), mean_err), normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an image classifier using Ground Truth labels\n",
    "At this stage, we have fully labeled our dataset and we can train a machine learning model to classify images based on the categories we previously defined. We'll do so using the **augmented manifest** output of our labeling job - no additional file translation or manipulation required! For a more complete description of the augmented manifest, see our other [example notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/ground_truth_labeling_jobs/object_detection_augmented_manifest_training/object_detection_augmented_manifest_training.ipynb).\n",
    "\n",
    "**NOTE:** Training neural networks to high accuracy often requires a careful choice of hyperparameters. In this case, we hand-picked hyperparameters that work reasonably well for this dataset. The neural net should have accuracy of about **60% with 100 datapoints, but it could go  over 95% if you're more than 1000 datapoints with auto-labeling.**\n",
    "\n",
    "First, we'll split our augmented manifest into a training set and a validation set using an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.manifest', 'r') as f:\n",
    "    output = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "# Shuffle output in place.\n",
    "np.random.shuffle(output)\n",
    "    \n",
    "dataset_size = len(output)\n",
    "train_test_split_index = round(dataset_size*0.8)\n",
    "\n",
    "train_data = output[:train_test_split_index]\n",
    "validation_data = output[train_test_split_index:]\n",
    "\n",
    "num_training_samples = 0\n",
    "with open('train.manifest', 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        num_training_samples += 1\n",
    "    \n",
    "with open('validation.manifest', 'w') as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll upload these manifest files to the previously defined S3 bucket so that they can be used in the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('train.manifest',BUCKET, EXP_NAME + '/train.manifest')\n",
    "s3.upload_file('validation.manifest',BUCKET, EXP_NAME + '/validation.manifest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique job name \n",
    "nn_job_name_prefix = 'groundtruth-augmented-manifest-demo'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "nn_job_name = nn_job_name_prefix + timestamp\n",
    "\n",
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'image-classification', repo_version='latest')\n",
    "\n",
    "training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"Pipe\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": 's3://{}/{}/output/'.format(BUCKET, EXP_NAME)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,   \n",
    "        \"InstanceType\": \"ml.p3.2xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": nn_job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"epochs\": \"30\",\n",
    "        \"image_shape\": \"3,224,224\",\n",
    "        \"learning_rate\": \"0.01\",\n",
    "        \"lr_scheduler_step\": \"10,20\",\n",
    "        \"mini_batch_size\": \"32\",\n",
    "        \"num_classes\": str(num_classes),\n",
    "        \"num_layers\": \"18\",\n",
    "        \"num_training_samples\": str(num_training_samples),\n",
    "        \"resize\": \"224\",\n",
    "        \"use_pretrained_model\": \"1\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": 's3://{}/{}/{}'.format(BUCKET, EXP_NAME, 'train.manifest'),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": [\"source-ref\",\"category\"]\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": 's3://{}/{}/{}'.format(BUCKET, EXP_NAME, 'validation.manifest'),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": [\"source-ref\",\"category\"]\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_client.create_training_job(**training_params)\n",
    "\n",
    "# Confirm that the training job has started\n",
    "print('Transform job started')\n",
    "while(True):\n",
    "    status = sagemaker_client.describe_training_job(TrainingJobName=nn_job_name)['TrainingJobStatus']\n",
    "    if status == 'Completed':\n",
    "        print(\"Transform job ended with status: \" + status)\n",
    "        break\n",
    "    if status == 'Failed':\n",
    "        message = response['FailureReason']\n",
    "        print('Transform failed with the following error: {}'.format(message))\n",
    "        raise Exception('Transform job failed') \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model \n",
    "\n",
    "Now that we've fully labeled our dataset and have a trained model, we want to use the model to perform inference.\n",
    "\n",
    "Image classification only supports encoded .jpg and .png image formats as inference input for now. The output is the probability values for all classes encoded in JSON format, or in JSON Lines format for batch transform.\n",
    "\n",
    "This section involves several steps,\n",
    "\n",
    "    Create Model - Create model for the training output\n",
    "    Batch Transform - Create a transform job to perform batch inference.\n",
    "    Host the model for realtime inference - Create an inference endpoint and perform realtime inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_name=\"groundtruth-demo-ic-model\" + timestamp\n",
    "print(model_name)\n",
    "info = sagemaker_client.describe_training_job(TrainingJobName=nn_job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': training_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform\n",
    "We now create a SageMaker Batch Transform job using the model created above to perform batch prediction.\n",
    "\n",
    "### Download Test Data\n",
    "First, let's download a test image that has been held out from the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "batch_job_name = \"image-classification-model\" + timestamp\n",
    "batch_input = 's3://{}/{}/test/'.format(BUCKET, EXP_NAME)\n",
    "batch_output = 's3://{}/{}/{}/output/'.format(BUCKET, EXP_NAME, batch_job_name)\n",
    "\n",
    "# Copy two images from each class, unseen by the neural net, to a local bucket.\n",
    "test_images = []\n",
    "for class_id in ['/m/04szw', '/m/02xwb', '/m/0cd4d', '/m/07dm6', '/m/0152hh']:\n",
    "    test_images.extend([label[0] + '.jpg' for label in all_labels if (label[2] == class_id and label[3] == '1')][-2:])\n",
    "    \n",
    "!aws s3 rm $batch_input --recursive\n",
    "for test_img in test_images:\n",
    "    !aws s3 cp s3://open-images-dataset/test/{test_img} {batch_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \\\n",
    "{\n",
    "    \"TransformJobName\": batch_job_name,\n",
    "    \"ModelName\": model_name,\n",
    "    \"MaxConcurrentTransforms\": 16,\n",
    "    \"MaxPayloadInMB\": 6,\n",
    "    \"BatchStrategy\": \"SingleRecord\",\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": 's3://{}/{}/{}/output/'.format(BUCKET, EXP_NAME, batch_job_name)\n",
    "    },\n",
    "    \"TransformInput\": {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": batch_input\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"application/x-image\",\n",
    "        \"SplitType\": \"None\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    },\n",
    "    \"TransformResources\": {\n",
    "            \"InstanceType\": \"ml.p2.xlarge\",\n",
    "            \"InstanceCount\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print('Transform job name: {}'.format(batch_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_client.create_transform_job(**request)\n",
    "\n",
    "print(\"Created Transform job with name: \", batch_job_name)\n",
    "\n",
    "while(True):\n",
    "    response = sagemaker_client.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = response['TransformJobStatus']\n",
    "    if status == 'Completed':\n",
    "        print(\"Transform job ended with status: \" + status)\n",
    "        break\n",
    "    if status == 'Failed':\n",
    "        message = response['FailureReason']\n",
    "        print('Transform failed with the following error: {}'.format(message))\n",
    "        raise Exception('Transform job failed') \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes, let's inspect the prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(out_fname):\n",
    "    !aws s3 cp {out_fname} .\n",
    "    print(out_fname)\n",
    "    with open(out_fname.split('/')[-1]) as f:\n",
    "        data = json.load(f)\n",
    "        index = np.argmax(data['prediction'])\n",
    "        probability = data['prediction'][index]\n",
    "    print(\"Result: label - \" + CLASS_LIST[index] + \", probability - \" + str(probability))\n",
    "    input_fname = out_fname.split('/')[-1][:-4]\n",
    "    return CLASS_LIST[index], probability, input_fname\n",
    "\n",
    "# Show prediction results.\n",
    "!rm test_inputs/*\n",
    "plt.figure(facecolor='white', figsize=(7, 15), dpi=100)\n",
    "outputs = !aws s3 ls {batch_output}\n",
    "outputs = [get_label(batch_output + prefix.split()[-1]) for prefix in outputs]\n",
    "outputs.sort(key=lambda pred: pred[1], reverse=True)\n",
    "\n",
    "for fname_id, (pred_cname, pred_conf, pred_fname) in enumerate(outputs):\n",
    "    !aws s3 cp {batch_input}{pred_fname} test_inputs/{pred_fname}\n",
    "    plt.subplot(5, 2, fname_id+1) \n",
    "    img = imageio.imread('test_inputs/{}'.format(pred_fname))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('{}\\nconfidence={:.2f}'.format(pred_cname, pred_conf))\n",
    "    \n",
    "if RUN_FULL_AL_DEMO:\n",
    "    warning = ''\n",
    "else:\n",
    "    warning = ('\\nNOTE: In this small demo we only used 80 images to train the neural network.\\n'\n",
    "               'The predictions will be far from perfect! Set RUN_FULL_AL_DEMO=True to see properly trained results.')\n",
    "plt.suptitle('Predictions sorted by confidence.{}'.format(warning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime Inference\n",
    "\n",
    "We now host the model with an endpoint and perform realtime inference.\n",
    "\n",
    "This section involves several steps,\n",
    "\n",
    "    Create endpoint configuration - Create a configuration defining an endpoint.\n",
    "    Create endpoint - Use the configuration to create an inference endpoint.\n",
    "    Perform inference - Perform inference on some input data using the endpoint.\n",
    "    Clean up - Delete the endpoint and model\n",
    "\n",
    "## Create Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name + '-epc' + timestamp\n",
    "endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint\n",
    "\n",
    "Lastly, the customer creates the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes about 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name + '-ep' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker_client.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))\n",
    "\n",
    "# get the status of the endpoint\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "# wait until the status has changed\n",
    "sagemaker_client.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_inputs/{}'.format(test_images[0]), 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "\n",
    "# `response` comes in a json format, let's unpack it.\n",
    "result = json.loads(response['Body'].read())\n",
    "\n",
    "img = imageio.imread('test_inputs/{}'.format(test_images[0]))\n",
    "plt.imshow(img)\n",
    "# The result outputs the probabilities for all classes.\n",
    "# Find the class with maximum probability and print the class name.\n",
    "print('Model prediction is: {}'.format(CLASS_LIST[np.argmax(result)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's clean up and delete this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "We covered a lot of ground in this notebook! Let's recap what we accomplished. First we started with an unlabeled dataset (technically, the dataset was previously labeled by the authors of the dataset, but we discarded the original labels for the purposes of this demonstration). Next, we created a SageMake Ground Truth labeling job and generated new labels for all of the images in our dataset. Then we split this file into a training set and a validation set and trained a SageMaker image classification model. Finally, we created a hosted model endpoint and used it to make a live prediction for a held-out image in the original dataset.\n",
    "\n",
    "This tutorial is a condense version of longer and fuller tutorial that shows you how to label 1000 images with Groud Truth public workforce and auto-labeling feature with much higher accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
